{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "06fcab79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['akiapo', 'aniani', 'apapan', 'barpet', 'crehon', 'elepai', 'ercfra', 'hawama', 'hawcre', 'hawgoo', 'hawhaw', 'hawpet1', 'houfin', 'iiwi', 'jabwar', 'maupar', 'omao', 'puaioh', 'skylar', 'warwhe1', 'yefcan']\n",
      "21\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import fastai\n",
    "f = open(\"D:\\\\Tasks\\\\BIRD_CLEF\\\\birdclef-2022\\\\scored_birds.json\")\n",
    "T = json.load(f)\n",
    "L=list(T)\n",
    "print(L)\n",
    "print(len(L))\n",
    "print(type(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d5f4f3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 6, 7, 9, 44, 46, 47, 60, 62, 63, 64, 65, 67, 70, 72, 90, 101, 111, 131, 141, 150]\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "  \n",
    "# Get the list of all files and directories\n",
    "# in the root directory\n",
    "path = \"/\"\n",
    "dir_list = os.listdir(\"birdclef-2022\\\\train_audio\")\n",
    "bird_map = []\n",
    "for idx,dir1 in enumerate(dir_list):\n",
    "    if(dir1 in L):\n",
    "        bird_map.append(idx)\n",
    "print(bird_map)\n",
    "print(len(bird_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ebc53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import torchaudio\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.utils import save_image\n",
    "base_folder = Path('birdclef-2022')\n",
    "train = pd.read_csv('birdclef-2022/train_metadata.csv')\n",
    "items = get_files(base_folder, extensions='.ogg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "802761a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "class BirdClefDataset(Dataset):\n",
    "\n",
    "    def __init__(self, annotations_file, audio_dir,transformation):\n",
    "        self.train_meta = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.target_sample_rate = 32000\n",
    "        self.device = \"cuda\"\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.num_samples=1800\n",
    "        self.num_classes = self._create_onehot()\n",
    "\n",
    "    def _create_label(self,bird):\n",
    "        lbl = [0]*self.num_classes\n",
    "        lbl[self.enc[bird]] = 1\n",
    "        return lbl[:]\n",
    "    \n",
    "    def depredict(self,label,bird):\n",
    "        return label[self.enc[bird]]\n",
    "    \n",
    "    def _create_onehot(self):\n",
    "        classes = set()\n",
    "        self.enc = {} ##\n",
    "        for i in self.train_meta.primary_label:\n",
    "            classes.add(i)\n",
    "        for idx,clss in enumerate(classes):\n",
    "            self.enc[clss] = idx\n",
    "        return len(classes)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_meta)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        #signal -> (num_channels,samples) -> (1,sr)\n",
    "#         signal = self._resample_if_necessary(signal,sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._rightpad_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal, torch.from_numpy(np.array(self._create_label(label),dtype=np.float32)).to(device)\n",
    "    \n",
    "    def _resample_if_necessary(self,signal,sr):\n",
    "        if sr!= self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr,self.target_sample_rate)\n",
    "            signal = resample(signal)\n",
    "        return signal\n",
    "    def _mix_down_if_necessary(self,signal): # turn multiple channels into mono, do we have multiple channels?\n",
    "        if(signal.shape[0]>1):\n",
    "            signal = torch.mean(input = signal, dim=0,keepdim=True)\n",
    "        return signal\n",
    "    def _cut_if_necessary(self,signal):\n",
    "        # signal(tensor)[num_channels=1,number of samples]\n",
    "        if(signal.shape[1]>self.num_samples):\n",
    "            signal = signal[:,:self.num_samples]\n",
    "        return signal\n",
    "    def _rightpad_if_necessary(self,signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if(length_signal < self.num_samples):\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0,num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal,last_dim_padding)\n",
    "        return signal\n",
    "        \n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        fold = f\"fold{self.train_meta.iloc[index, 0]}\"\n",
    "        bird,file = self.train_meta.iloc[index, 12].split(\"/\")\n",
    "        path = os.path.join(self.audio_dir,bird,file)\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.train_meta.iloc[index, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3724420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "random_seed =  9\n",
    "SAMPLE_RATE = 32000\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE\n",
    "                                                      ,n_fft=1024,\n",
    "                                                      hop_length=512,\n",
    "                                                      n_mels= 64)\n",
    "dataset = BirdClefDataset(\"D:\\\\Tasks\\\\BIRD_CLEF\\\\birdclef-2022\\\\train_metadata.csv\",\n",
    "                            \"birdclef-2022\\\\train_audio\", mel_spectrogram)\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "validation_split = 0.2\n",
    "shuffle_dataset = True\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=200,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "89fb679e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "train_meta = pd.read_csv(\"D:\\\\Tasks\\\\BIRD_CLEF\\\\birdclef-2022\\\\train_metadata.csv\")\n",
    "print(len(train_meta.primary_label.unique()))\n",
    "print(dataset._create_label(\"comgal1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "915be14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images has shape:  torch.Size([40, 1, 64, 4])\n",
      "Batch of labels has shape:  torch.Size([40, 152])\n"
     ]
    }
   ],
   "source": [
    "for imgs, labels in train_loader:\n",
    "    print(\"Batch of images has shape: \",imgs.size())\n",
    "    print(\"Batch of labels has shape: \", labels.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcbd463",
   "metadata": {},
   "source": [
    "for imgs, labels in data_loader:\n",
    "    print(\"Batch of images has shape: \",imgs.shape)\n",
    "    print(\"Batch of labels has shape: \", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b7ed2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdClefModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(256,256) # simple dense layer\n",
    "            , nn.ReLU(),\n",
    "            nn.Linear(256,152)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self,input_data):\n",
    "        flattened_data = self.flatten(input_data)\n",
    "        logits = self.dense_layers(flattened_data)\n",
    "        predictions = self.softmax(logits)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4ef34d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(12)\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(12)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv4 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(24)\n",
    "        self.conv5 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(24)\n",
    "        self.fc1 = nn.Linear(24*10*10, 152)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = F.relu(self.bn1(self.conv1(input)))      \n",
    "        output = F.relu(self.bn2(self.conv2(output)))     \n",
    "        output = self.pool(output)                        \n",
    "        output = F.relu(self.bn4(self.conv4(output)))     \n",
    "        output = F.relu(self.bn5(self.conv5(output)))     \n",
    "        output = output.view(-1, 24*10*10)\n",
    "        output = self.fc1(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812cff75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1653405413.6072958\n",
      "Epoch 0:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e8bf64cdca41aeb819679aa6f1e44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "device ='cuda'\n",
    "def train_one_epoch(model,data_loader,loss_fn,optimiser,device):\n",
    "    device ='cuda'\n",
    "    for inputs,targets in tqdm(data_loader):\n",
    "        inputs,targets = inputs.to(device),targets.to(device)\n",
    "        #Loss calculations\n",
    "        predictions = model(inputs)\n",
    "        #calculate loss and backpropagate , using gd for updating \n",
    "        loss = loss_fn(predictions,targets)\n",
    "    \n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "     \n",
    "def train(model,data_loader,loss_fn,optimiser,device,epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i}:\")\n",
    "        train_one_epoch(model,data_loader,loss_fn,optimiser,device)\n",
    "        print(\"------------\")\n",
    "    print(\"Training is done\")\n",
    "\n",
    "\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "#loss \n",
    "loss_fn = nn.functional.cross_entropy\n",
    "model = Network().to('cuda')\n",
    "import time\n",
    "print(time.time())\n",
    "start = time.time()\n",
    "optimiser = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "train(model,train_loader,loss_fn,optimiser,device,EPOCHS)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "333bd4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(),\"LRUmodel_1.pth\")\n",
    "model_val = BirdClefModel()\n",
    "trained_state_dict = torch.load(\"LRUmodel_1.pth\")\n",
    "model_val.load_state_dict(trained_state_dict)\n",
    "# validation_loader = torch.utils.data.DataLoader(dataset, batch_size=2,\n",
    "#                                                 sampler=valid_sampler)\n",
    "# valid_loss = 0.0\n",
    "# model.eval()     # Optional when not using Model Specific layer\n",
    "# for data, labels in validation_loader:\n",
    "#     if torch.cuda.is_available():\n",
    "#         data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "#     target = model(data)\n",
    "#     print(target.shape)\n",
    "#     print(labels.shape)\n",
    "#     loss = loss_fn(target,labels)\n",
    "#     valid_loss = loss.item() * data.size(0)\n",
    "#     print(loss)\n",
    "\n",
    "# print(f'Epoch {e+1} \\t\\t Training Loss: {train_loss / len(trainloader)} \\t\\t Validation Loss: {valid_loss / len(validloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ba3b7ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exporting 0.ogg\n",
      "exporting 1.ogg\n",
      "exporting 2.ogg\n",
      "exporting 3.ogg\n",
      "exporting 4.ogg\n",
      "exporting 5.ogg\n",
      "exporting 6.ogg\n",
      "exporting 7.ogg\n",
      "exporting 8.ogg\n",
      "exporting 9.ogg\n",
      "exporting 10.ogg\n",
      "exporting 11.ogg\n"
     ]
    }
   ],
   "source": [
    "#Here we split the test audio file\n",
    "test_audio_dir = 'birdclef-2022/test_soundscapes/'\n",
    "file_list = [f.split('.')[0] for f in sorted(os.listdir(test_audio_dir))]\n",
    "with open('birdclef-2022/scored_birds.json') as sbfile:\n",
    "    scored_birds = json.load(sbfile)\n",
    "pred = {'row_id': [], 'target': []}\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import make_chunks \n",
    "sound = AudioSegment.from_file(\"birdclef-2022/test_soundscapes/soundscape_453028782.ogg\")\n",
    "\n",
    "chunk_length_ms = 5000 # pydub calculates in millisec \n",
    "chunks = make_chunks(sound,chunk_length_ms)\n",
    "for i, chunk in enumerate(chunks): \n",
    "    chunk_name = \"{0}.ogg\".format(i) \n",
    "    print (\"exporting\", chunk_name) \n",
    "    chunk.export(chunk_name, format=\"ogg\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ff93fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def _resample_if_necessary(signal,sr):\n",
    "        if sr!= 32000:\n",
    "            resampler = torchaudio.transforms.Resample(sr,32000)\n",
    "            signal = resample(signal)\n",
    "        return signal\n",
    "def _mix_down_if_necessary(signal): # turn multiple channels into mono, do we have multiple channels?\n",
    "    if(signal.shape[0]>1):\n",
    "        signal = torch.mean(input = signal, dim=0,keepdim=True)\n",
    "    return signal\n",
    "def _cut_if_necessary(signal):\n",
    "    # signal(tensor)[num_channels=1,number of samples]\n",
    "    if(signal.shape[1]>1800):\n",
    "        signal = signal[:,:1800]\n",
    "    return signal\n",
    "\n",
    "def features_extractor(file):\n",
    "    transformation = mel_spectrogram.to('cuda')\n",
    "    signal, sr = torchaudio.load(file)\n",
    "    signal = signal.to('cuda')\n",
    "    #signal -> (num_channels,samples) -> (1,sr)\n",
    "    signal = _resample_if_necessary(signal,sr)\n",
    "    signal = _mix_down_if_necessary(signal)\n",
    "    signal = _cut_if_necessary(signal)\n",
    "    signal = transformation(signal)\n",
    "    return signal\n",
    "\n",
    "\n",
    "import os\n",
    "extracted_features_=[]\n",
    "for i in range(0,12):\n",
    "    file_name = os.path.join(os.path.abspath(\"./\"),str(i) + \".ogg\")\n",
    "    data=features_extractor(file_name)\n",
    "    extracted_features_.append([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a8ac9f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "extracted_features_df_=pd.DataFrame(extracted_features_,columns=['feature'])\n",
    "X_pro = extracted_features_df_['feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6f483953",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('birdclef-2022/scored_birds.json') as sbfile:\n",
    "    scored_birds = json.load(sbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f5bcc0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predict__=model( torch.cat(X_pro.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5f2801e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is where we will store our results\n",
    "pred = {'row_id': [], 'target': []}\n",
    "\n",
    "# Process audio files and make predictions\n",
    "bird_map = [3, 6, 7, 9, 44, 46, 47, 60, 62, 63, 64, 65, 67, 70, 72, 90, 101, 111, 131, 141, 150]\n",
    "for afile in file_list:\n",
    "    \n",
    "    # Complete file path\n",
    "    path = test_audio_dir + afile + '.ogg'\n",
    "    \n",
    "    # Open file with librosa and split signal into 5-second chunks\n",
    "    # sig, rate = librosa.load(path)\n",
    "    # ...\n",
    "    \n",
    "    # Let's assume we have a list of 12 audio chunks (1min / 5s == 12 segments)\n",
    "    chunks = [[] for i in range(12)]\n",
    "    \n",
    "    # Make prediction for each chunk\n",
    "    # Each scored bird gets a random value in our case\n",
    "    # since we don't actually have a model\n",
    "    for i in range(len(chunks)):        \n",
    "        chunk_end_time = (i + 1) * 5\n",
    "        j=0\n",
    "        for bird in scored_birds:\n",
    "            \n",
    "            # This is our random prediction score for this bird\n",
    "            score = predict__[i][j]\n",
    "            j=j+1\n",
    "            # Assemble the row_id which we need to do for each scored bird\n",
    "            row_id = afile + '_' + bird + '_' + str(chunk_end_time)\n",
    "            # Put the result into our prediction dict and\n",
    "            # apply a \"confidence\" threshold of 0.5\n",
    "            pred['row_id'].append(row_id)\n",
    "            pred['target'].append(True if score > 0.00006 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2815eac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068f7d65d0c54960b8604a4bd7a6e4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.026534557342529\n",
      "------------\n",
      "Epoch 1:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1822a6c1b5642b880c58d115339d4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.033028602600098\n",
      "------------\n",
      "Epoch 2:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e18cd7829b14108afbc7f96ce787a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.029596328735352\n",
      "------------\n",
      "Epoch 3:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfaeba8fc36c4bf1a43463b7bb2bf782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.893874645233154\n",
      "------------\n",
      "Epoch 4:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25bb473eb4949d084a81b8abee074d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.0327277183532715\n",
      "------------\n",
      "Epoch 5:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11befbac2ba545cb82319d86f35d2c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.03239631652832\n",
      "------------\n",
      "Epoch 6:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc50b010d474768b92f9cbf035ca6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.033878326416016\n",
      "------------\n",
      "Epoch 7:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787e12f98cb44bea8916b969860d855c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.944440841674805\n",
      "------------\n",
      "Epoch 8:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142beb38958148f49bba50a88ad067bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.032506942749023\n",
      "------------\n",
      "Epoch 9:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb97a9944da49a19f81b119b9c4986b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.027227878570557\n",
      "------------\n",
      "Training is done\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "device ='cuda'\n",
    "def train_one_epoch(model,data_loader,loss_fn,optimiser,device):\n",
    "    device ='cuda'\n",
    "    for inputs,targets in tqdm(data_loader):\n",
    "        inputs,targets = inputs.to(device),targets.to(device)\n",
    "        #Loss calculations\n",
    "        while(inputs.shape[2]<5):\n",
    "            inputs = torch.cat([inputs,[0]*64])\n",
    "        predictions = model(inputs)\n",
    "        #calculate loss and backpropagate , using gd for updating \n",
    "        loss = loss_fn(predictions,targets)\n",
    "    \n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "     \n",
    "def train(model,data_loader,loss_fn,optimiser,device,epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i}:\")\n",
    "        train_one_epoch(model,data_loader,loss_fn,optimiser,device)\n",
    "        print(\"------------\")\n",
    "    print(\"Training is done\")\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "#loss \n",
    "loss_fn = nn.functional.cross_entropy\n",
    "model = BirdClefModel().to('cuda')\n",
    "optimiser = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "train(model,train_loader,loss_fn,optimiser,device,EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "69379f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          row_id  target\n",
      "0  soundscape_453028782_akiapo_5   False\n",
      "1  soundscape_453028782_aniani_5   False\n",
      "2  soundscape_453028782_apapan_5   False\n",
      "3  soundscape_453028782_barpet_5   False\n",
      "4  soundscape_453028782_crehon_5   False\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(pred, columns = ['row_id', 'target'])\n",
    "\n",
    "# Quick sanity check\n",
    "print(results.head()) \n",
    "    \n",
    "# Convert our results to csv\n",
    "results.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
